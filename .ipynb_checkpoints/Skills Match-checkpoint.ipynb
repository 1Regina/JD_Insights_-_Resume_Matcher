{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Import packages and libraries\n",
    "# Import libraries and packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import textract\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize,RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"We are looking for a data scientist that will help us discover the information hidden in vast amounts of data, and help us make smarter decisions to deliver even better products. Your primary focus will be in applying data mining techniques, doing statistical analysis, and building high quality prediction systems integrated with our products. automate scoring using machine learning techniques, build recommendation systems, improve and extend the features used by our existing classifier, develop internal A/B testing procedures, build system for automated fraud detection.\\r\\nResponsibilities Selecting features, building and optimizing classifiers using machine learning techniques Data mining using state-of-the-art methods Extending company's data with third party sources of information when needed Enhancing data collection procedures to include information that is relevant for building analytic systems Processing, cleansing, and verifying the integrity of data used for analysis Doing ad-hoc analysis and presenting results in a clear manner Creating automated anomaly detection systems and constant tracking of its performance\\r\\nSkills and Qualifications Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc. Experience with common data science toolkits, such as R, Weka, NumPy, MatLab. Excellence in at least one of these is highly desirable Great communication skills Experience with data visualisation tools, such as D3.js, GGplot, etc. Proficiency in using query languages such as SQL, Hive, Pig Experience with NoSQL databases, such as MongoDB, Cassandra, HBase Good applied statistics skills, such as distributions, statistical testing, regression, etc. Good scripting and programming skills Data-oriented personality\\r\\n\\r\\n\\x0c\"\n"
     ]
    }
   ],
   "source": [
    "#%% load the first doc (JD)\n",
    "text0 = textract.process(r\"C:\\Users\\regina\\Desktop\\Metis\\Metis Projects\\JDTopics\\Resume data\\Data Scientist JD.pdf\")\n",
    "str(text0)\n",
    "print(text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Jonathan Whitmore PhD, Data Scientist\\n\\nMountain View, CA +1 650-943-3715 B JBWhit@gmail.com \\xc3\\x8d JonathanWhitmore.com JBWhit JonathanBWhitmoreExperience\\n\\n2014Present\\n\\nData Scientist, Silicon Valley Data Science, Mountain View, CA, USA. { Consulting as a member of several small data science/data engineering teams at multiple companies. { Creatingoutputtoexplaindataanalysis, datavisualization, andstatisticalmodelingresultstomanagers. { Modeling survey data responses with ordinal logistic regression in R. { Analyzing and visualizing user behavior migration. 2014 Insight Fellow, Insight Data Science, Palo Alto, CA, USA. { Created a Data Science project to predict the auction sale price of Abstract Expressionist art. 2011\\xe2\\x80\\x932014 Postdoctoral Research Associate, Swinburne University, Melbourne, AUS. { Cleaned noisy and inhomogeneous astronomical data taken over four years by di\\xef\\xac\\x80erent observing groups. { Curated central data repository of \\xef\\xac\\x81nal products; developed an automated process to update data repository; created web interface for collaborator access to the repository. { Utilized numerous statistical techniques, including sensitivity analysis on non-linear propagation of errors, Markov-Chain Monte Carlo for model building, and hypothesis testing via information criterion. { Simulatedspectroscopicdatatoexposesystematicerrorsthatchallengelong-standingresultsonwhether the fundamental physical constants of the universe are constant. 2005\\xe2\\x80\\x932011 Graduate Student Researcher, UCSD, San Diego, CA, USA. { Developed a novel technique to extract information from high resolution spectroscopic data that led to uncovering unknown short-range systematic errors. Programming and Development Skills Languages Python, SQL (Impala/Hive), R, L ATEX, shell scripts, CSS, HTML. Tools Jupyter Notebook, pandas, matplotlib, numpy, scikit-learn, scipy, pymc3, git, pandoc. Publishing, Speaking, and Side Projects 2016 O\\xe2\\x80\\x99Reilly author: Jupyter Notebook for Data Science Teams [screencast], editor O\\xe2\\x80\\x99Reilly Media. 2016 UC Berkeley Guest Lecturer: Master in Data Science lecture on Jupyter Notebook. 2015 Open Source Speaker: OSCON. 2014\\xe2\\x80\\x932015 Technical reviewer of Mastering SciPy by Francisco J. Blanco-Silva, 2015. 2012\\xe2\\x80\\x932014 DeveloperofRebalanceAssetAllocation,aPythonmodulethatrecommends\\xef\\xac\\x81nancialassetclassallocations. 2013-2014 Contributor to astropy; creator of dipole_error, an astronomy Python module. 2013 Co-star and narrator of Hidden Universe, a 3D IMAX astronomy \\xef\\xac\\x81lm playing worldwide. 2007\\xe2\\x80\\x932009 Graduate Physics Courses Taken: Stochastic Methods, Computational Physics. Education 2011 PhD Physics, University of California San Diego, San Diego, CA, USA. 2007 MS Physics, University of California San Diego, San Diego, CA, USA. 2005 Bachelor of Science\\xe2\\x80\\x93Magna Cum Laude, Vanderbilt University, Nashville, TN, USA. Triple major: Physics (honors); Mathematics; Philosophy.'\n"
     ]
    }
   ],
   "source": [
    "#%% load the 2nd doc\n",
    "text1 = textract.process(r\"C:\\Users\\regina\\Desktop\\Metis\\Metis Projects\\JDTopics\\Resume data\\Whitmore.docx\")\n",
    "str(text1)\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Khristian Kotov CERN, CH-1211 Geneva 23, Switzerland; kkotov@cern.ch; +41 (0) 76 451 1507\\n\\nProfessional summary: I am a data scientist and software engineer with a substantial experience in big (petabyte-scale) data handling and processing. My work relies heavily on implementing e\\xef\\xac\\x83cient program algorithms, running Monte Carlo modeling, and performing statistical inference. I routinely use machine learning techniques, architect data analysis pipelines, and publish in the major peer-reviewed journals.\\n\\nHighlights: Data mining, cleaning, and imputing, principal component analysis, statistical data modeling Pattern recognition, forecasting, classi\\xef\\xac\\x81cation, clustering, regression/\\xef\\xac\\x81tting Greedy algorithms, e\\xef\\xac\\x83cient data structures, dynamic programming Parallel computing, High-Throughput Computing, computing clusters and grids\\n\\nTechnical skills: Full pro\\xef\\xac\\x81ciency in C/C++/STL programming Strong background in Python, SQL, R (including writing packages), regular expressions Experience with Apache Spark, Linux administration\\n\\nExperience: 2015 Aug \\xe2\\x80\\x93 present: Scienti\\xef\\xac\\x81c software engineer at the University of Florida (UF) Took over an orphan database logging system recording hardware con\\xef\\xac\\x81guration and running conditions and extended its functionality to address new requirements of the hardware upgrade\\n\\n2010 Oct \\xe2\\x80\\x93 2015 July: Data Scientist at The Ohio State University (formally Ph.D. Research Associate)\\n\\nProposed, designed, performed, and published topical physics analyses at LHC CERN [1,2,3,4]\\n\\n- conducted exploratory analysis, identi\\xef\\xac\\x81ed relevant classi\\xef\\xac\\x81ers - designed work\\xef\\xac\\x82ows for processing large datasets on LHC computing grid and clusters - performed statistical modeling, drawn quantitative conclusions, wrote scienti\\xef\\xac\\x81c publications\\n\\n2004 Jan \\xe2\\x80\\x93 2010 Sep: Ph.D. Research Assistant at the University of Florida\\n\\nAssisted production and commissioning of the custom-built electronic system [5] for fast online pattern recognition. Prepared and performed physics analyses with the very \\xef\\xac\\x81rst data from LHC.\\n\\n- designed suites for the data quality monitoring [6,7] - evaluated performance of the system by analyzing data from the dedicated tests - measured the underlying event model parameters in the new energy regime of the LHC\\n\\n1998 Sep \\xe2\\x80\\x93 2003 Dec: Research Assistant at Budker Institute of Nuclear Physics\\n\\nDeveloped software tools for data analysis at the detector KEDR; here I implemented:\\n\\n- analysis framework structuring numerous loosely connected Fortran and C/C++ libraries - machine learning based particle identi\\xef\\xac\\x81cation (decision trees, neural networks [rus]) - algorithms for energy measurement calibration [rus] (corrected 30% error at the start-up)\\n\\nEducation: Ph.D. in Physics, University of Florida, Gainesville, USA, 2010 (4.0 GPA) M.S. (2001) and B.S. (1999) in Physics with honor, Novosibirsk State University, Russia\\n\\nCoursera certi\\xef\\xac\\x81cates in: Data Science [1,2,3,4], Statistical Inference, Machine Learning [1,2], Algorithms, Bioinformatics [1,2,3,4]'\n"
     ]
    }
   ],
   "source": [
    "#%% load the 3rd doc\n",
    "text2 = textract.process(r\"C:\\Users\\regina\\Desktop\\Metis\\Metis Projects\\JDTopics\\Resume data\\Khristian Kotov.docx\")\n",
    "str(text2)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Regina Cheong  https://github.com/1Regina |  https://www.linkedin.com/in/regina-cheong/  \\n\\n Summary  A seasoned professional experienced in analytics and extraction of insights, focusing on applications in finance, corporate governance and financial services.  Conversant in SQL, Python for packages such as NumPy, Pandas, Matplotlib, Seaborn, Machine Learning and visualization platforms such as Tableau and Power BI. \\n\\n Currently seeking a Business Intelligence / Data Scientist role to deepen my data skillset while leveraging my commercial experience. \\n\\n Career Experiences \\n\\n Metis  Data Scientist                                                                \\t\\t\\t\\t Jul \\xe2\\x80\\x9819 \\xe2\\x80\\x93 Present \\n\\nAttending a 12 weeks immersive Data Science Programme with applications involving multi-facets of statistics, project design, data transformation, machine learning and story-telling. \\n\\nProduced various supervised and unsupervised learning projects involving regression, classification (logistic regression, KNN, Na\\xc3\\xafve Bayes, SVM, Decision Forests, bagging, ensemble), natural language processing, and deep learning. \\n\\nProjects include: Using machine learning for investment decisions; rainfall predictor. \\n\\n SMRT Corporation Financial Planning & Analysis Manager \\t\\t\\t   Feb \\xe2\\x80\\x93 Jun \\xe2\\x80\\x9919 [Contract] \\n\\nAnalyzed the group\\xe2\\x80\\x99s P&L performance during the Qtr4 and Financial Year closing to provide insights to management using key metrices e.g. ridership, fare, energy consumption, headcount, fleet size. \\n\\nReviewed overseas investment for impairment using scenario testing. \\n\\nEnterprise Singapore Senior Business Partner \\t\\t\\t\\t\\t\\tJul \\xe2\\x80\\x9817 \\xe2\\x80\\x93 Jul \\xe2\\x80\\x9818 Analyzed different clusters performance to highlight trends and insights with charts (e.g. scatter plot, bar charts). \\n\\nAided KPIs negotiation using local business landscape analysis. \\xef\\x82\\xb7 Partnered MTI to draft local champions transformation stories for minister\\xe2\\x80\\x99s speeches.  \\n\\nAchievement: Alignment of multiple business groups requirements unto a new common CRM platform resulting in cost savings of $142k on subscriptions. (2018). \\n\\n KPMG Manager  \\t\\t\\t\\t\\t\\t\\t\\t        \\tOct \\xe2\\x80\\x9815 \\xe2\\x80\\x93 Jul \\xe2\\x80\\x9917 \\n\\nProject management for thought leadership initiatives (e.g. resource planning, review, data analysis and charts).  \\n\\nSupported research and analysis efforts (e.g. survey questions, invites, desktop research, response analysis). Achievements: Two publications on i) review of 545 listed companies\\xe2\\x80\\x99 annual report (AR) disclosure and ii) comparative study of 200 companies\\xe2\\x80\\x99 AR disclosures over a 3 years gap. \\n\\n Nanyang Polytechnic Senior Internal Auditor\\t\\t\\t\\t\\t\\t  Aug \\xe2\\x80\\x9809 \\xe2\\x80\\x93 Jan \\xe2\\x80\\x9812 \\n\\nConducted audits and wrote reports to support procedural enhancements for management consideration. \\n\\n Morgan Stanley Product Controller\\t\\t\\t\\t\\t\\t\\t Jul \\xe2\\x80\\x9807 \\xe2\\x80\\x93 Mar \\xe2\\x80\\x9809 \\n\\nAnalyzed daily profit and loss (P&L) key drivers for reporting after validation to news sources. \\n\\nAchievement: Chief Financial Officer Excellence Award \\n\\nCredit Suisse Product Controller \\t\\t\\t\\t\\t\\t\\tMay \\xe2\\x80\\x9806 \\xe2\\x80\\x93 Jul \\xe2\\x80\\x9807\\n\\nReconciled daily profit and loss (P&L) for reporting. \\n\\n PricewaterhouseCoopers Senior Associate \\t\\t\\t\\t\\t\\tSep \\xe2\\x80\\x9803 \\xe2\\x80\\x93 Apr \\xe2\\x80\\x9806  \\n\\nResearched industry trends for variance analysis evaluation.  \\n\\nManaged projects from planning to completion including coaching junior staff, project deliverables and budget. \\n\\n Skills  \\n\\nData visualization: Tableau, Power BI, Matplotlib, Seaborn, GGplot\\n\\n Programming Language: Python Machine Learning: Regression, Classification, NLP, Deep-Learning  \\n\\nDatabase technologies: SQL. Mongo Statistical tools: NumPy \\n\\nMachine Learning Algo-NN, Naive Bayes, SVM, Decision Forest\\n\\nApplied statistics skill: Regression, Classification and NLP\\n\\nMicrosoft Office: Excel, PowerPoint, Word \\n\\n \\n\\nQualifications \\n\\nUdemy Completed 8 courses spanning 60 hours in data visualization (Tableau & Power BI), Python, SQL (2017 -Present) \\n\\nChartered Accountant | ISCA (Institute of Singapore Chartered Accountants) (2005 \\xe2\\x80\\x93 Present)\\n\\n Bachelor of Accountancy |Nanyang Technological University (2000 \\xe2\\x80\\x93 2003)'\n"
     ]
    }
   ],
   "source": [
    "#%% load the 4th doc\n",
    "text3 = textract.process(r\"C:\\Users\\regina\\Desktop\\Metis\\Metis Projects\\JDTopics\\Resume data\\Regina Cheong summary.docx\")\n",
    "str(text3)\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with a single column of strings of the 4 docs\n",
    "data = {'description': [text0,text1,text2,text3]}\n",
    "df_description = pd.DataFrame(data, columns = ['description'])\n",
    "df_description\n",
    "df_description.to_csv(\"df_description.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_description0= pd.read_csv(r\"C:\\Users\\regina\\Desktop\\Metis\\Metis Projects\\JD Resume Matcher\\job_description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_description0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"We are looking for a data scientist that wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Jonathan Whitmore PhD, Data Scientist\\n\\nMou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Khristian Kotov CERN, CH-1211 Geneva 23, Swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Regina Cheong  https://github.com/1Regina | ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  b\"We are looking for a data scientist that wil...\n",
       "1  b'Jonathan Whitmore PhD, Data Scientist\\n\\nMou...\n",
       "2  b'Khristian Kotov CERN, CH-1211 Geneva 23, Swi...\n",
       "3  b'Regina Cheong  https://github.com/1Regina | ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description= pd.read_csv(r\"df_description.csv\")\n",
    "df_description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>no_punctuation</th>\n",
       "      <th>lower_case</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>key_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"We are looking for a data scientist that wil...</td>\n",
       "      <td>bWe are looking for a data scientist that will...</td>\n",
       "      <td>bwe are looking for a data scientist that will...</td>\n",
       "      <td>[bwe, are, looking, for, a, data, scientist, t...</td>\n",
       "      <td>[bwe, looking, data, scientist, help, us, disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Jonathan Whitmore PhD, Data Scientist\\n\\nMou...</td>\n",
       "      <td>bJonathan Whitmore PhD Data ScientistnnMountai...</td>\n",
       "      <td>bjonathan whitmore phd data scientistnnmountai...</td>\n",
       "      <td>[bjonathan, whitmore, phd, data, scientistnnmo...</td>\n",
       "      <td>[bjonathan, whitmore, phd, data, scientistnnmo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Khristian Kotov CERN, CH-1211 Geneva 23, Swi...</td>\n",
       "      <td>bKhristian Kotov CERN CH1211 Geneva 23 Switzer...</td>\n",
       "      <td>bkhristian kotov cern ch1211 geneva 23 switzer...</td>\n",
       "      <td>[bkhristian, kotov, cern, ch1211, geneva, 23, ...</td>\n",
       "      <td>[bkhristian, kotov, cern, ch1211, geneva, 23, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Regina Cheong  https://github.com/1Regina | ...</td>\n",
       "      <td>bRegina Cheong  httpsgithubcom1Regina   httpsw...</td>\n",
       "      <td>bregina cheong  httpsgithubcom1regina   httpsw...</td>\n",
       "      <td>[bregina, cheong, httpsgithubcom1regina, https...</td>\n",
       "      <td>[bregina, cheong, httpsgithubcom1regina, https...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  b\"We are looking for a data scientist that wil...   \n",
       "1  b'Jonathan Whitmore PhD, Data Scientist\\n\\nMou...   \n",
       "2  b'Khristian Kotov CERN, CH-1211 Geneva 23, Swi...   \n",
       "3  b'Regina Cheong  https://github.com/1Regina | ...   \n",
       "\n",
       "                                      no_punctuation  \\\n",
       "0  bWe are looking for a data scientist that will...   \n",
       "1  bJonathan Whitmore PhD Data ScientistnnMountai...   \n",
       "2  bKhristian Kotov CERN CH1211 Geneva 23 Switzer...   \n",
       "3  bRegina Cheong  httpsgithubcom1Regina   httpsw...   \n",
       "\n",
       "                                          lower_case  \\\n",
       "0  bwe are looking for a data scientist that will...   \n",
       "1  bjonathan whitmore phd data scientistnnmountai...   \n",
       "2  bkhristian kotov cern ch1211 geneva 23 switzer...   \n",
       "3  bregina cheong  httpsgithubcom1regina   httpsw...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [bwe, are, looking, for, a, data, scientist, t...   \n",
       "1  [bjonathan, whitmore, phd, data, scientistnnmo...   \n",
       "2  [bkhristian, kotov, cern, ch1211, geneva, 23, ...   \n",
       "3  [bregina, cheong, httpsgithubcom1regina, https...   \n",
       "\n",
       "                                           key_words  \n",
       "0  [bwe, looking, data, scientist, help, us, disc...  \n",
       "1  [bjonathan, whitmore, phd, data, scientistnnmo...  \n",
       "2  [bkhristian, kotov, cern, ch1211, geneva, 23, ...  \n",
       "3  [bregina, cheong, httpsgithubcom1regina, https...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation \n",
    "df_description['no_punctuation'] = df_description['description'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Change to lower case\n",
    "df_description ['lower_case'] = df_description['no_punctuation'].astype(str).str.lower()\n",
    "\n",
    "# Apply word tokenizer\n",
    "df_description['tokenized_text'] = df_description['lower_case'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "df_description['key_words'] =df_description['tokenized_text'].apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n",
    "\n",
    "df_description.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>no_punctuation</th>\n",
       "      <th>lower_case</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>key_words</th>\n",
       "      <th>joined_Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"We are looking for a data scientist that wil...</td>\n",
       "      <td>bWe are looking for a data scientist that will...</td>\n",
       "      <td>bwe are looking for a data scientist that will...</td>\n",
       "      <td>[bwe, are, looking, for, a, data, scientist, t...</td>\n",
       "      <td>[bwe, looking, data, scientist, help, us, disc...</td>\n",
       "      <td>bwe looking data scientist help us discover in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Jonathan Whitmore PhD, Data Scientist\\n\\nMou...</td>\n",
       "      <td>bJonathan Whitmore PhD Data ScientistnnMountai...</td>\n",
       "      <td>bjonathan whitmore phd data scientistnnmountai...</td>\n",
       "      <td>[bjonathan, whitmore, phd, data, scientistnnmo...</td>\n",
       "      <td>[bjonathan, whitmore, phd, data, scientistnnmo...</td>\n",
       "      <td>bjonathan whitmore phd data scientistnnmountai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Khristian Kotov CERN, CH-1211 Geneva 23, Swi...</td>\n",
       "      <td>bKhristian Kotov CERN CH1211 Geneva 23 Switzer...</td>\n",
       "      <td>bkhristian kotov cern ch1211 geneva 23 switzer...</td>\n",
       "      <td>[bkhristian, kotov, cern, ch1211, geneva, 23, ...</td>\n",
       "      <td>[bkhristian, kotov, cern, ch1211, geneva, 23, ...</td>\n",
       "      <td>bkhristian kotov cern ch1211 geneva 23 switzer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Regina Cheong  https://github.com/1Regina | ...</td>\n",
       "      <td>bRegina Cheong  httpsgithubcom1Regina   httpsw...</td>\n",
       "      <td>bregina cheong  httpsgithubcom1regina   httpsw...</td>\n",
       "      <td>[bregina, cheong, httpsgithubcom1regina, https...</td>\n",
       "      <td>[bregina, cheong, httpsgithubcom1regina, https...</td>\n",
       "      <td>bregina cheong httpsgithubcom1regina httpswwwl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  b\"We are looking for a data scientist that wil...   \n",
       "1  b'Jonathan Whitmore PhD, Data Scientist\\n\\nMou...   \n",
       "2  b'Khristian Kotov CERN, CH-1211 Geneva 23, Swi...   \n",
       "3  b'Regina Cheong  https://github.com/1Regina | ...   \n",
       "\n",
       "                                      no_punctuation  \\\n",
       "0  bWe are looking for a data scientist that will...   \n",
       "1  bJonathan Whitmore PhD Data ScientistnnMountai...   \n",
       "2  bKhristian Kotov CERN CH1211 Geneva 23 Switzer...   \n",
       "3  bRegina Cheong  httpsgithubcom1Regina   httpsw...   \n",
       "\n",
       "                                          lower_case  \\\n",
       "0  bwe are looking for a data scientist that will...   \n",
       "1  bjonathan whitmore phd data scientistnnmountai...   \n",
       "2  bkhristian kotov cern ch1211 geneva 23 switzer...   \n",
       "3  bregina cheong  httpsgithubcom1regina   httpsw...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [bwe, are, looking, for, a, data, scientist, t...   \n",
       "1  [bjonathan, whitmore, phd, data, scientistnnmo...   \n",
       "2  [bkhristian, kotov, cern, ch1211, geneva, 23, ...   \n",
       "3  [bregina, cheong, httpsgithubcom1regina, https...   \n",
       "\n",
       "                                           key_words  \\\n",
       "0  [bwe, looking, data, scientist, help, us, disc...   \n",
       "1  [bjonathan, whitmore, phd, data, scientistnnmo...   \n",
       "2  [bkhristian, kotov, cern, ch1211, geneva, 23, ...   \n",
       "3  [bregina, cheong, httpsgithubcom1regina, https...   \n",
       "\n",
       "                                         joined_Sent  \n",
       "0  bwe looking data scientist help us discover in...  \n",
       "1  bjonathan whitmore phd data scientistnnmountai...  \n",
       "2  bkhristian kotov cern ch1211 geneva 23 switzer...  \n",
       "3  bregina cheong httpsgithubcom1regina httpswwwl...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the tokenized words for count vectorizing\n",
    "df_description['joined_Sent'] = [' '.join(map(str, indStem)) for indStem in df_description['key_words']]\n",
    "df_description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description.to_csv(\"df_description_senten.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def breakwords (column):\n",
    "#     for senten in df_description['key_words']:\n",
    "#         words = senten.split()\n",
    "#         return words\n",
    "        \n",
    "# Taking the first 80 words after target word in dataframe column\n",
    "def nextword(target, source):\n",
    "    skillslist = []\n",
    "    for i, w in enumerate(source):\n",
    "#         print(i,w)\n",
    "        # Find index where target is present in w\n",
    "        # Then return the 80 words from that index.\n",
    "        for j,indword in enumerate(w):\n",
    "            if target == indword:\n",
    "                print(j)\n",
    "                skillslist.append(w[j+1:j+81])\n",
    "                break\n",
    "    return skillslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "160\n",
      "82\n",
      "353\n"
     ]
    }
   ],
   "source": [
    "skills = nextword ('skills' ,df_description['key_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experience', 'data', 'visualisation', 'tools', 'd3js', 'ggplot', 'etc', 'proficiency', 'using', 'query', 'languages', 'sql', 'hive', 'pig', 'experience', 'nosql', 'databases', 'mongodb', 'cassandra', 'hbase', 'good', 'applied', 'statistics', 'skills', 'distributions', 'statistical', 'testing', 'regression', 'etc', 'good', 'scripting', 'programming', 'skills', 'dataoriented', 'personalityrnrnx0c']\n"
     ]
    }
   ],
   "source": [
    "print(skills[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_skills0 = ' '.join(skills[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experience data visualisation tools d3js ggplot etc proficiency using query languages sql hive pig experience nosql databases mongodb cassandra hbase good applied statistics skills distributions statistical testing regression etc good scripting programming skills dataoriented personalityrnrnx0c'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_skills0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_skills1 = ' '.join(skills[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_skills2 = ' '.join(skills[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_skills3 = ' '.join(skills[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a For Loop instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_skill = [] # initiate the list outside\n",
    "for n in range (0,4):\n",
    "    x = ' '.join(skills[n])\n",
    "    str_skill.append(x)\n",
    "\n",
    "type(str_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience data visualisation tools d3js ggplot etc proficiency using query languages sql hive pig experience nosql databases mongodb cassandra hbase good applied statistics skills distributions statistical testing regression etc good scripting programming skills dataoriented personalityrnrnx0c',\n",
       " 'languages python sql impalahive r l atex shell scripts css html tools jupyter notebook pandas matplotlib numpy scikitlearn scipy pymc3 git pandoc publishing speaking side projects 2016 oxe2x80x99reilly author jupyter notebook data science teams screencast editor oxe2x80x99reilly media 2016 uc berkeley guest lecturer master data science lecture jupyter notebook 2015 open source speaker oscon 2014xe2x80x932015 technical reviewer mastering scipy francisco j blancosilva 2015 2012xe2x80x932014 developerofrebalanceassetallocationapythonmodulethatrecommendsxefxacx81nancialassetclassallocations 20132014 contributor astropy creator dipole_error astronomy python module 2013 costar narrator hidden universe 3d imax',\n",
       " 'full proxefxacx81ciency ccstl programming strong background python sql r including writing packages regular expressions experience apache spark linux administrationnnexperience 2015 aug xe2x80x93 present scientixefxacx81c software engineer university florida uf took orphan database logging system recording hardware conxefxacx81guration running conditions extended functionality address new requirements hardware upgradenn2010 oct xe2x80x93 2015 july data scientist ohio state university formally phd research associatennproposed designed performed published topical physics analyses lhc cern 1234nn conducted exploratory analysis identixefxacx81ed relevant classixefxacx81ers designed workxefxacx82ows processing large datasets lhc',\n",
       " 'nndata visualization tableau power bi matplotlib seaborn ggplotnn programming language python machine learning regression classification nlp deeplearning nndatabase technologies sql mongo statistical tools numpy nnmachine learning algonn naive bayes svm decision forestnnapplied statistics skill regression classification nlpnnmicrosoft office excel powerpoint word nn nnqualifications nnudemy completed 8 courses spanning 60 hours data visualization tableau power bi python sql 2017 present nnchartered accountant isca institute singapore chartered accountants 2005 xe2x80x93 presentnn bachelor accountancy nanyang technological university 2000 xe2x80x93 2003']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([str_skills0,str_skills1,str_skills2,str_skills3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dataframe with a single column of strings of the 4 docs\n",
    "# data = {'skills': [str_skills0,str_skills1,str_skills2,str_skills3]}\n",
    "# df_skills = pd.DataFrame(data, columns = ['skills'])\n",
    "# df_skills\n",
    "# df_skills.to_csv(\"df_skills.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experience data visualisation tools d3js ggplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>languages python sql impalahive r l atex shell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>full proxefxacx81ciency ccstl programming stro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nndata visualization tableau power bi matplotl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              skills\n",
       "0  experience data visualisation tools d3js ggplo...\n",
       "1  languages python sql impalahive r l atex shell...\n",
       "2  full proxefxacx81ciency ccstl programming stro...\n",
       "3  nndata visualization tableau power bi matplotl..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with a single column of strings of the 4 docs\n",
    "data = {'skills': str_skill }\n",
    "df_skills = pd.DataFrame(data, columns = ['skills'])\n",
    "df_skills\n",
    "# df_skills.to_csv(\"df_skills.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experience data visualisation tools d3js ggplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>languages python sql impalahive r l atex shell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>full proxefxacx81ciency ccstl programming stro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nndata visualization tableau power bi matplotl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              skills\n",
       "0  experience data visualisation tools d3js ggplo...\n",
       "1  languages python sql impalahive r l atex shell...\n",
       "2  full proxefxacx81ciency ccstl programming stro...\n",
       "3  nndata visualization tableau power bi matplotl..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1234nn</th>\n",
       "      <th>2000</th>\n",
       "      <th>2003</th>\n",
       "      <th>2005</th>\n",
       "      <th>2012xe2x80x932014</th>\n",
       "      <th>2013</th>\n",
       "      <th>20132014</th>\n",
       "      <th>2014xe2x80x932015</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>...</th>\n",
       "      <th>universe</th>\n",
       "      <th>university</th>\n",
       "      <th>upgradenn2010</th>\n",
       "      <th>using</th>\n",
       "      <th>visualisation</th>\n",
       "      <th>visualization</th>\n",
       "      <th>word</th>\n",
       "      <th>workxefxacx82ows</th>\n",
       "      <th>writing</th>\n",
       "      <th>xe2x80x93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1234nn  2000  2003  2005  2012xe2x80x932014  2013  20132014  \\\n",
       "0       0     0     0     0                  0     0         0   \n",
       "1       0     0     0     0                  1     1         1   \n",
       "2       1     0     0     0                  0     0         0   \n",
       "3       0     1     1     1                  0     0         0   \n",
       "\n",
       "   2014xe2x80x932015  2015  2016  ...  universe  university  upgradenn2010  \\\n",
       "0                  0     0     0  ...         0           0              0   \n",
       "1                  1     2     2  ...         1           0              0   \n",
       "2                  0     2     0  ...         0           2              1   \n",
       "3                  0     0     0  ...         0           1              0   \n",
       "\n",
       "   using  visualisation  visualization  word  workxefxacx82ows  writing  \\\n",
       "0      1              1              0     0                 0        0   \n",
       "1      0              0              0     0                 0        0   \n",
       "2      0              0              0     0                 1        1   \n",
       "3      0              0              2     1                 0        0   \n",
       "\n",
       "   xe2x80x93  \n",
       "0          0  \n",
       "1          0  \n",
       "2          2  \n",
       "3          2  \n",
       "\n",
       "[4 rows x 209 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document-Term Matrix (Count Vectorizer)\n",
    "word_vectorizer = CountVectorizer(analyzer='word',stop_words = 'english')\n",
    "sparse_matrix = word_vectorizer.fit_transform(df_skills[\"skills\"])\n",
    "dt_skills = pd.DataFrame(sparse_matrix.toarray(), columns=word_vectorizer.get_feature_names())\n",
    "# dt_skills.shape\n",
    "dt_skills.tail()\n",
    "# dt_skills.to_csv(\"dt_skills.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Document-Term Matrix - TF-IDF [NOTES IS INCORRECT due to array.shape]\n",
    "# cv_tfidf = TfidfVectorizer(ngram_range=(1,2), analyzer='word')\n",
    "# sparse_matrix = cv_tfidf.fit_transform(df_skills[\"skills\"]) # include all documents\n",
    "# dt_tfidf = pd.DataFrame(sparse_matrix.toarray(), columns=cv_tfidf.get_feature_names())\n",
    "# dt_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Cosine Similarity##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "# calculate the cosine similarity between all combinations of documents\n",
    "from itertools import combinations\n",
    "\n",
    "# corpus = [str(\" \".join(skills[0])), str(\" \".join(skills[1])),str(\" \".join(skills[2])),str(\" \".join(skills[3]))]   \n",
    "\n",
    "#list all of the combinations of 4 take 2 as well as the pairs of phrases\n",
    "pairs =list(combinations(enumerate(str_skill),2))\n",
    "combos =[(a[0], b[0]) for a,b in pairs] # documents pairs\n",
    "phrases =[(a[1],b[1]) for a, b in pairs] # words pairs\n",
    "print(combos)\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07927524902574902"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formula of cosine simiarity\n",
    "np.dot(np.array(dt_skills.iloc[0])/np.sqrt(np.sum(np.square(np.array(dt_skills.iloc[0])))),\n",
    "       np.array(dt_skills.iloc[1]/np.sqrt(np.sum(np.square(np.array(dt_skills.iloc[1]))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "# results_comparison = [cosine_similarity(dt_skills.iloc[a],dt_skills.iloc[b]) for a, b in combos] \n",
    "results_comparison = [np.dot(np.array(dt_skills.iloc[a])/np.sqrt(np.sum(np.square(np.array(dt_skills.iloc[a])))),\n",
    "       np.array(dt_skills.iloc[b]/np.sqrt(np.sum(np.square(np.array(dt_skills.iloc[b])))))) for a, b in combos]\n",
    "#sorted(zip(results_comparison,phrases),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)],\n",
       " [0.07927524902574902,\n",
       "  0.08486775178136235,\n",
       "  0.14708710135363806,\n",
       "  0.09445988668039029,\n",
       "  0.11116216147498596,\n",
       "  0.14064125478257256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combos,results_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF COSINE similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put comparatives into corpus of strings and apply cosine similarity##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [str(skills[0]), str(skills[1]),str(skills[2]),str(skills[3])]                                                                                                                                                                                                   \n",
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       \n",
    "pairwise_similarity = tfidf * tfidf.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.03319965, 0.04233082, 0.08069443],\n",
       "       [0.03319965, 1.        , 0.04760509, 0.0541448 ],\n",
       "       [0.04233082, 0.04760509, 1.        , 0.09339443],\n",
       "       [0.08069443, 0.0541448 , 0.09339443, 1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.03319965, 0.04233082, 0.08069443])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
